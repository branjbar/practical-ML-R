---
title: "Assessment of Sport Activities - Project Writup"
output: html_document
date: "`r format(Sys.time(), '%d %B, %Y')`"
---

[View in GitHub](https://github.com/branjbar/practical-ML-R/tree/gh-pages)

# Abstract
In this writeup, I'm reporting my work on the *Assessment of Sport Activities* project of *Practical Machine Learning* course. By using a combination of manual explorations of data, and an efficient use of a decision tree, I was able to use just 10 infomative features and train a *Random Forest* to reach accuracy of 0.98. According to this precision, I expect the out of sample error to be about 0.98, as well. For the 20 verification samples, I expected 0.98 * 20 = 19.6 correct answers; indeed all my answers turned out to be correct according to the course website. 

# Background 
Six young health participants were asked to perform one set of 10 repetitions 
of the Unilateral Dumbbell Biceps Curl (UDBC) in five different fashions: exactly 
according to the specification (Class A), throwing the elbows to the front (Class B), 
lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) 
and throwing the hips to the front (Class E). For further reading we refer to <http://groupware.les.inf.puc-rio.br/har>

The aim of this project is to train a prediction model which can analyse a UDBC activity using the available features and assess its quality as A, B, C, D or E.


In the following, we start by understanding the problem at hand, then we continue by importing the data, and exploring the large featureset. Once a small set of important features are recognized we train a Random Forest model, and test it with the available datset. Finally the available verification data for 20 sample activities are use the evalute the prediction model; the model shows 100% accuracy in verifications. 

# Understanding the Problem at Hand
In order to understand the problem at hand, we should know what type of activity UDBC is. The following link is a nice totorual for this activity by an expert. 

<iframe width="560" height="315" src="https://www.youtube.com/embed/YxtwA7XRK_g" frameborder="0" allowfullscreen></iframe>

# Initialization
Let's set a seed value to make sure the expertiments are repeatable. The packages *lattice*, *ggplot2*, *rattle* and *caret* are needed for this code.
```{r results='hide', message=FALSE}
set.seed(3421)
library(lattice)
library(ggplot2)
library(caret)
library(rattle)
```

### Loading data
Before loading data, make sure the working directory is pointing to where the *pml-training.csv* is located. It's important to translate the *NA* and *#DIV/0!* strings in data import to the *NA* value known by R. 
```{r echo=FALSE}
setwd("/Users/bian/sandbox/practical-ML-R/")
```
```{r}
myData <- read.csv("data/pml-training.csv", 
                   header=T,  
                   na.strings=c("NA", "#DIV/0!"))
```

Also import the Validation data which consists of 20 sample activites. 
```{r}
validationData <- read.csv("data/pml-testing.csv")
```

### Seperate the Training and Testing Data
Here we split the *Data* into training and testing sets, where 70% of data is assigned to training set.
```{r}
inTrain <- createDataPartition(y=myData$classe,
                               p=.7,
                               list=FALSE)
training <- myData[inTrain,]
testing <- myData[-inTrain,]
```

###  Exploratory Analytics
For exploring the data, as the number of features is very large, I use the *Feature Plot* of Caret to visualize the features and find the most informative piars. For instance, in following feature plot, we see that the selected features which are *roll_arm*, *pitch_arm*, *pitch_belt*, and *roll_belt* to some extent can seperate differnt classes.
```{r}
featurePlot(x=training[,c("roll_arm","pitch_arm","pitch_belt","roll_belt")],
				y=training$classe,
				plot="pairs")
```

I did this manual exploration of data for almost two hours, which was fun and helped me to understand the data much better. Eventually, I chose the following set of 21 features and the *classe* output as the *useful_features*; I beleive this subset introduces the most informative features in data. 
### Choosing Informative Features
```{r}
useful_features = c("pitch_belt","roll_belt","yaw_belt","total_accel_belt",
                    "gyros_belt_y", "gyros_belt_z","accel_belt_y","magnet_belt_y",
                    "roll_arm","pitch_arm","yaw_arm","avg_roll_arm","accel_arm_x",
                    "avg_roll_dumbbell","gyros_dumbbell_x","magnet_dumbbell_x",
                    "accel_dumbbell_y","magnet_dumbbell_z",
                    "roll_forearm","max_roll_forearm","min_pitch_forearm","classe")
```

Correspondingly, I define the *training_useful* and *testing_useful* datasets.
```{r}

training_useful <- training[,useful_features]
testing_useful <- testing[,useful_features]
```


####  Imputing the NA Values
As many of the values are missing among the useful features, we need to do some impuation. Thus, the following.
```{r}
preObj <- preProcess(training_useful[,-22], method="knnImpute") 
knnTrain <- predict(preObj, training_useful[,-22])
knnTrain$classe <- training_useful$classe
```

In this stage, I use a Decision Tree with aim of finding a smaller subset of informative features. Hopefully, Decision tree will pick a small subset of features for the decision tree. 
```{r result='hide', message=FALSE}
folds=2;repeats=2
fitControl <- trainControl(method="repeatedcv",number=folds,repeats=repeats,
                           classProbs=T,allowParallel=T)

modelFit <- train(classe~., data=knnTrain,
                  method="rpart",metric="ROC",
                  tuneLength=10,
                  trControl=fitControl)
```
Let's visualize the tree and find the important features. 
```{r}
fancyRpartPlot(modelFit$finalModel)
```


Yess! Although, the accuracy of this tree is low (about 60%) it's only using 10 featueres. This suggests that the other features are not informative enough.

### Pronning the Feature Set
Inspired by the shortlisted features after training a *tree* in previous section, I redefine the *useful_features* as following.
``` {r}
useful_features = c("roll_belt","max_roll_forearm","yaw_belt","magnet_dumbbell_z",
                    "accel_dumbbell_y", "pitch_belt","accel_belt_y","magnet_dumbbell_x",
                    "magnet_belt_y","magnet_dumbbell_z","classe")

training_useful <- training[,useful_features]
testing_useful <- testing[,useful_features]

```
For the new feature set perform the KNN Imputation.
```{r}
preObj <- preProcess(training_useful[,-11], method="knnImpute") 
knnTrain <- predict(preObj, training_useful[,-11])
knnTrain$classe <- training_useful$classe
```

# Training 
Now, we train a Random Forest classifier which we expect to have a very high accuracy.
```{r meassage=FALSE, results='hide'}

modelFit <- train(classe~., data=knnTrain,
                  method="rf")

modelFit$finalModel
```

Yessss, the results are very promissing, and the model has a high accuracy. 

# Testing
Here we use testing dataset to test the model. Before, that we use the knn imputation to replace the not available data values in testing set.
```{r}
knnTest <- predict(preObj, testing_useful[,-11])
confusionM <- confusionMatrix(predict(modelFit,knnTest), testing_useful$classe)  
confusionM
```


# Validation
In order to validate the results, we apply the KNN imputation on the feature sets, and make predictions of the validation data provided in Course Project.
```{r}
validating <- validationData[useful_features[-11]]
knnValid <- predict(preObj, validating[,-11])
answers = predict(modelFit,knnValid)
```

### Submiting Answers
To submit the answers, following code puts each prediction in a file with appropriate name.
```{r}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("data/problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(answers)

```
